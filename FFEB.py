# -*- coding: utf-8 -*-
"""Untitled58.ipynb

Automatically generated by Colab.




# asd_pipeline_full_with_custom_image.py
# Full pipeline for TF 2.19: FFEB + 3 individual classifiers + soft voting (comparison)
# Includes profiling (params, training time, GPU mem, CPU/RAM), domain-shift tests, custom-image prediction.
#
# Run in Colab after mounting Drive:
from google.colab import drive
drive.mount('/content/drive')

import os
import time
import json
import random
import shutil
import psutil
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import layers, models, applications, backend as K
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import cv2
import warnings
warnings.filterwarnings('ignore')

# ----------------------------
# USER CONFIG - change as needed
# ----------------------------
TRAIN_DIR = '/content/drive/MyDrive/train'
VAL_DIR   = '/content/drive/MyDrive/valid'
TEST_DIR  = '/content/drive/MyDrive/test'

OUT_DIR = '/content/drive/MyDrive/ASD_Output'   # chosen output folder
os.makedirs(OUT_DIR, exist_ok=True)

IMG_SIZE = (224, 224)
BATCH_SIZE = 16
EPOCHS = 15
LEARNING_RATE = 1e-4
CLASS_NAMES = ['autistic', 'non_autistic']

SEED = 42                # single seed; change or run multiple times for stats
SKIP_TRAINING = False    # if True tries to load models from OUT_DIR
TRAIN_INDIVIDUALS = True # whether to train individual backbone classifiers (for soft-voting)
USE_GPU = True
SAVE_PLOTS = True

# reproducibility
def set_all_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

set_all_seeds(SEED)

# GPU config
gpus = tf.config.list_physical_devices('GPU')
if gpus and USE_GPU:
    try:
        for g in gpus:
            tf.config.experimental.set_memory_growth(g, True)
        print("GPUs available:", len(gpus))
    except Exception as e:
        print("GPU config error:", e)
else:
    print("No GPU or GPU disabled. Running on CPU.")

# ----------------------------
# Helpers: JSON safe convert, profiling, domain-shift
# ----------------------------
def convert_numpy(obj):
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    if isinstance(obj, dict):
        return {k: convert_numpy(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [convert_numpy(i) for i in obj]
    return obj

def get_gpu_memory():
    info = {}
    try:
        gpus = tf.config.list_physical_devices('GPU')
        if not gpus:
            info['gpu_available'] = False
            return info
        info['gpu_available'] = True
        mem = tf.config.experimental.get_memory_info('GPU:0')
        info['gpu_current_bytes'] = int(mem.get('current', 0))
        info['gpu_peak_bytes'] = int(mem.get('peak', 0))
    except Exception as e:
        info['gpu_available'] = False
        info['error'] = str(e)
    return info

def get_system_usage():
    cpu = psutil.cpu_percent(interval=0.5)
    vm = psutil.virtual_memory()
    return {'cpu_percent': cpu, 'ram_total_gb': round(vm.total/1024**3,2), 'ram_used_gb': round(vm.used/1024**3,2),
            'ram_percent': vm.percent}

def model_size_on_disk(path):
    if os.path.exists(path):
        return os.path.getsize(path)
    return None

# Domain shift transforms (images in [0,1])
def add_gaussian_noise(image, std=0.03):
    noise = np.random.normal(0, std, image.shape)
    return np.clip(image + noise, 0., 1.)

def occlude_random_square(image, max_size_ratio=0.25):
    h, w, _ = image.shape
    size = int(min(h,w) * random.uniform(0.08, max_size_ratio))
    x = random.randint(0, w-size)
    y = random.randint(0, h-size)
    image[y:y+size, x:x+size, :] = 0.0
    return image

def apply_domain_shift(img, mode='brightness'):
    out = img.copy()
    if mode == 'brightness':
        factor = random.uniform(0.4, 1.6)
        out = np.clip(out * factor, 0, 1)
    elif mode == 'blur':
        k = random.choice([3,5])
        out = cv2.blur((out*255).astype(np.uint8),(k,k))/255.0
    elif mode == 'color_shift':
        out[:,:,0] = np.clip(out[:,:,0]*random.uniform(0.6,1.4),0,1)
    elif mode == 'occlusion':
        out = occlude_random_square(out, max_size_ratio=0.35)
    elif mode == 'noise':
        out = add_gaussian_noise(out, std=random.uniform(0.01,0.06))
    return out

# ----------------------------
# Data loaders
# ----------------------------
def get_generators(seed=SEED, augment=True):
    if augment:
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=15,
            width_shift_range=0.08,
            height_shift_range=0.08,
            shear_range=0.08,
            zoom_range=0.08,
            horizontal_flip=True,
            brightness_range=(0.7, 1.3),
            fill_mode='nearest'
        )
    else:
        train_datagen = ImageDataGenerator(rescale=1./255)

    val_datagen = ImageDataGenerator(rescale=1./255)
    test_datagen = ImageDataGenerator(rescale=1./255)

    train_gen = train_datagen.flow_from_directory(
        TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,
        classes=CLASS_NAMES, class_mode='binary', shuffle=True, seed=seed
    )
    val_gen = val_datagen.flow_from_directory(
        VAL_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,
        classes=CLASS_NAMES, class_mode='binary', shuffle=False, seed=seed
    )
    test_gen = test_datagen.flow_from_directory(
        TEST_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,
        classes=CLASS_NAMES, class_mode='binary', shuffle=False, seed=seed
    )
    return train_gen, val_gen, test_gen

# ----------------------------
# Model builders
# ----------------------------
def build_backbone(name, input_shape=(224,224,3), weights='imagenet'):
    inp = layers.Input(shape=input_shape)
    if name == 'vgg19':
        base = applications.VGG19(include_top=False, weights=weights, input_tensor=inp)
    elif name == 'resnet50':
        base = applications.ResNet50(include_top=False, weights=weights, input_tensor=inp)
    elif name == 'inception_v3':
        base = applications.InceptionV3(include_top=False, weights=weights, input_tensor=inp)
    else:
        raise ValueError('Unknown backbone')
    feat = layers.GlobalAveragePooling2D()(base.output)
    model = models.Model(inputs=base.input, outputs=feat, name=f'{name}_fe')
    last_conv = None
    for layer in base.layers[::-1]:
        if 'conv' in layer.name or 'mixed' in layer.name:
            last_conv = layer.name
            break
    return model, base, last_conv

def build_individual_classifier(name, input_shape=(224,224,3), freeze_base=True):
    fe, base, last_conv = build_backbone(name, input_shape=input_shape)
    if freeze_base:
        base.trainable = False
    inp = layers.Input(shape=input_shape)
    feat = fe(inp)
    x = layers.Dense(128, activation='relu')(feat)
    x = layers.Dropout(0.4)(x)
    out = layers.Dense(1, activation='sigmoid')(x)
    model = models.Model(inputs=inp, outputs=out, name=f'{name}_clf')
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
                  loss='binary_crossentropy', metrics=['accuracy'])
    return model, base, last_conv

def build_ffeb_model(input_shape=(224,224,3), freeze_backbones=True):
    vgg_fe, vgg_base, vgg_last = build_backbone('vgg19', input_shape=input_shape)
    res_fe, res_base, res_last = build_backbone('resnet50', input_shape=input_shape)
    inc_fe, inc_base, inc_last = build_backbone('inception_v3', input_shape=input_shape)

    if freeze_backbones:
        for base in [vgg_base, res_base, inc_base]:
            base.trainable = False

    inp = layers.Input(shape=input_shape, name='image_input')
    f1 = vgg_fe(inp)
    f2 = res_fe(inp)
    f3 = inc_fe(inp)

    PROJ_DIM = 512
    p1 = layers.Dense(PROJ_DIM, activation='relu', name='proj_vgg')(f1)
    p2 = layers.Dense(PROJ_DIM, activation='relu', name='proj_res')(f2)
    p3 = layers.Dense(PROJ_DIM, activation='relu', name='proj_inc')(f3)

    concat = layers.Concatenate(axis=1, name='concat_feats')([p1, p2, p3])
    resh = layers.Reshape((3, PROJ_DIM), name='reshape_feats')(concat)

    attn_logits = layers.Dense(1, name='attn_logits')(resh)
    attn_weights = layers.Softmax(axis=1, name='attn_weights')(attn_logits)
    weighted = layers.Multiply(name='weighted_feats')([resh, attn_weights])

    # ReduceSum (TF 2.19)
    try:
        fused = tf.keras.layers.ReduceSum(axis=1, name='fused_rep')(weighted)
    except Exception:
        fused = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1), name='fused_rep')(weighted)

    x = layers.Dense(256, activation='relu')(fused)
    x = layers.Dropout(0.4)(x)
    out = layers.Dense(1, activation='sigmoid', name='output')(x)

    model = models.Model(inputs=inp, outputs=out, name='FFEB_ASD')
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
                  loss='binary_crossentropy', metrics=['accuracy'])

    backbone_info = {
        'vgg': {'base': vgg_base, 'last_conv': vgg_last},
        'resnet': {'base': res_base, 'last_conv': res_last},
        'inception': {'base': inc_base, 'last_conv': inc_last}
    }
    return model, backbone_info

# ----------------------------
# Training & evaluation helpers
# ----------------------------
def train_model(model, train_gen, val_gen, run_name='model', epochs=EPOCHS):
    ckpt_path = os.path.join(OUT_DIR, f'{run_name}.h5')
    callbacks = [
        ModelCheckpoint(ckpt_path, monitor='val_accuracy', save_best_only=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),
        EarlyStopping(monitor='val_loss', patience=6, verbose=1, restore_best_weights=True)
    ]
    start = time.perf_counter()
    history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=callbacks, verbose=1)
    elapsed = time.perf_counter() - start
    if os.path.exists(ckpt_path):
        model.load_weights(ckpt_path)
    return history, elapsed

def evaluate_model_on_generator(model, gen):
    steps = int(np.ceil(gen.samples / gen.batch_size))
    preds = model.predict(gen, steps=steps, verbose=0)
    y_score = preds.ravel()
    y_pred = (y_score >= 0.5).astype(int)
    y_true = gen.classes[:len(y_pred)]
    return y_true, y_pred, y_score

def compute_metrics(y_true, y_pred, y_score=None):
    acc = float(accuracy_score(y_true, y_pred))
    prec = float(precision_score(y_true, y_pred, zero_division=0))
    rec = float(recall_score(y_true, y_pred, zero_division=0))
    f1 = float(f1_score(y_true, y_pred, zero_division=0))
    auc = float(roc_auc_score(y_true, y_score)) if (y_score is not None and len(np.unique(y_true))>1) else None
    cm = confusion_matrix(y_true, y_pred)
    if cm.size == 1:
        if y_true[0] == 0:
            tn = int(cm[0]); fp = fn = tp = 0
        else:
            tp = int(cm[0]); tn = fp = fn = 0
    else:
        tn, fp, fn, tp = cm.ravel()
    tpr = rec
    fpr = float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0
    specificity = float(tn / (tn + fp)) if (tn + fp) > 0 else 0.0
    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc,
            'confusion_matrix': cm.tolist(), 'TP': int(tp), 'TN': int(tn), 'FP': int(fp), 'FN': int(fn),
            'TPR': tpr, 'FPR': fpr, 'Specificity': specificity}

def soft_voting_predict(models_list, gen):
    steps = int(np.ceil(gen.samples / gen.batch_size))
    probs = []
    for m in models_list:
        p = m.predict(gen, steps=steps, verbose=0).ravel()
        probs.append(p[:gen.samples])
    avg = np.mean(probs, axis=0)
    y_pred = (avg >= 0.5).astype(int)
    y_true = gen.classes[:len(y_pred)]
    return y_true, y_pred, avg

# ----------------------------
# Plot history (accuracy/loss)
# ----------------------------
def plot_history(history, title_prefix, out_dir=OUT_DIR):
    try:
        fig, axs = plt.subplots(1,2,figsize=(12,4))
        axs[0].plot(history.history['accuracy'], label='train_acc')
        axs[0].plot(history.history['val_accuracy'], label='val_acc')
        axs[0].set_title(f'{title_prefix} Accuracy')
        axs[0].legend()
        axs[1].plot(history.history['loss'], label='train_loss')
        axs[1].plot(history.history['val_loss'], label='val_loss')
        axs[1].set_title(f'{title_prefix} Loss')
        axs[1].legend()
        fname = os.path.join(out_dir, f'{title_prefix}_history.png')
        plt.tight_layout()
        plt.savefig(fname)
        plt.close(fig)
        return fname
    except Exception as e:
        print('plot_history error:', e)
        return None

# ----------------------------
# Custom image prediction (single image path) - returns dict of results
# ----------------------------
def predict_custom_image(img_path, models_dict):
    """
    img_path: path of image file (RGB)
    models_dict: dict with keys: 'vgg', 'resnet', 'inception' (individual models), 'ffeb' (main model)
    returns: dictionary of model_name -> probability
    """
    if not os.path.exists(img_path):
        raise FileNotFoundError(f'Image not found: {img_path}')
    img = load_img(img_path, target_size=IMG_SIZE)
    x = img_to_array(img) / 255.0
    x_in = np.expand_dims(x, axis=0)
    out = {}
    # individual models
    for k in ['vgg19', 'resnet50', 'inception_v3']:
        m = models_dict.get(k)
        if m is None:
            out[k] = None
            continue
        p = float(m.predict(x_in, verbose=0).ravel()[0])
        out[k] = p
    # soft voting if all three present
    probs = [out[k] for k in ['vgg19','resnet50','inception_v3'] if out[k] is not None]
    out['soft_voting'] = float(np.mean(probs)) if len(probs)==3 else None
    # ffeb model
    ff = models_dict.get('ffeb')
    if ff is not None:
        pff = float(ff.predict(x_in, verbose=0).ravel()[0])
        out['ffeb'] = pff
    else:
        out['ffeb'] = None
    return out

# ----------------------------
# Main pipeline run
# ----------------------------
def run_pipeline(seed=SEED, skip_training=SKIP_TRAINING, train_individuals=TRAIN_INDIVIDUALS):
    set_all_seeds(seed)
    train_gen, val_gen, test_gen = get_generators(seed=seed, augment=True)
    results = {'seed': seed, 'models': {}, 'profiling': {}, 'domain_shift': {}}
    individuals = []
    individual_names = ['vgg19', 'resnet50', 'inception_v3']

    # TRAIN / LOAD individual models
    if train_individuals:
        if not skip_training:
            for name in individual_names:
                print(f'\nTraining individual classifier: {name}')
                m, base, last_conv = build_individual_classifier(name, input_shape=(IMG_SIZE[0],IMG_SIZE[1],3), freeze_base=True)
                tot = m.count_params()
                train_par = int(np.sum([K.count_params(w) for w in m.trainable_weights]))
                non_train = tot - train_par
                results['models'][f'{name}_pretrain_info'] = {'total_params': int(tot), 'trainable_params': train_par, 'non_trainable_params': int(non_train)}
                hist, t_elapsed = train_model(m, train_gen, val_gen, run_name=f'baseline_{name}_seed{seed}', epochs=EPOCHS)
                model_path = os.path.join(OUT_DIR, f'baseline_{name}_seed{seed}.h5')
                try:
                    m.save(model_path)
                except Exception as e:
                    print('Warning: failed to save', model_path, e)
                size = model_size_on_disk(model_path)
                y_true, y_pred, y_score = evaluate_model_on_generator(m, test_gen)
                metrics = compute_metrics(y_true, y_pred, y_score)
                metrics.update({'train_time_s': t_elapsed, 'model_path': model_path, 'model_size_bytes': size})
                results['models'][name] = convert_numpy(metrics)
                individuals.append(m)
                if SAVE_PLOTS:
                    try: plot_history(hist, f'{name}_training', out_dir=OUT_DIR)
                    except: pass
        else:
            for name in individual_names:
                model_path = os.path.join(OUT_DIR, f'baseline_{name}_seed{seed}.h5')
                if os.path.exists(model_path):
                    print('Loading', model_path)
                    m = tf.keras.models.load_model(model_path, compile=False)
                    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy'])
                    y_true, y_pred, y_score = evaluate_model_on_generator(m, test_gen)
                    metrics = compute_metrics(y_true, y_pred, y_score)
                    metrics.update({'train_time_s': None, 'model_path': model_path, 'model_size_bytes': os.path.getsize(model_path)})
                    results['models'][name] = convert_numpy(metrics)
                    individuals.append(m)
                else:
                    print('Warning: individual model file not found:', model_path)

    # soft voting
    if len(individuals) == 3:
        y_t, y_p, y_s = soft_voting_predict(individuals, test_gen)
        sv_metrics = compute_metrics(y_t, y_p, y_s)
        results['models']['soft_voting'] = convert_numpy(sv_metrics)
        print('\nSoft-voting metrics:', sv_metrics)
    else:
        print('Soft-voting not computed: need 3 trained individual models, found', len(individuals))

    # FFEB
    ffeb_path = os.path.join(OUT_DIR, f'FFEB_seed{seed}.h5')
    if not skip_training:
        print('\nTraining FFEB model (main)')
        ffeb_model, backbone_info = build_ffeb_model(input_shape=(IMG_SIZE[0],IMG_SIZE[1],3), freeze_backbones=True)
        tot = ffeb_model.count_params()
        train_par = int(np.sum([K.count_params(w) for w in ffeb_model.trainable_weights]))
        non_train = tot - train_par
        results['models']['FFEB_pretrain_info'] = {'total_params': int(tot), 'trainable_params': train_par, 'non_trainable_params': int(non_train)}
        hist_f, t_elapsed_ffeb = train_model(ffeb_model, train_gen, val_gen, run_name=f'FFEB_seed{seed}', epochs=EPOCHS)
        try:
            ffeb_model.save(ffeb_path)
        except Exception as e:
            print('Warning: failed to save FFEB model', ffeb_path, e)
        if SAVE_PLOTS:
            try: plot_history(hist_f, 'FFEB_training', out_dir=OUT_DIR)
            except: pass
    else:
        if os.path.exists(ffeb_path):
            print('Loading FFEB from', ffeb_path)
            ffeb_model = tf.keras.models.load_model(ffeb_path, compile=False)
            ffeb_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy'])
            _, backbone_info = build_ffeb_model(input_shape=(IMG_SIZE[0],IMG_SIZE[1],3), freeze_backbones=True)
            t_elapsed_ffeb = None
        else:
            raise FileNotFoundError('FFEB model not found; set skip_training=False to train')

    # Evaluate FFEB
    y_true_f, y_pred_f, y_score_f = evaluate_model_on_generator(ffeb_model, test_gen)
    ffeb_metrics = compute_metrics(y_true_f, y_pred_f, y_score_f)
    ffeb_metrics.update({'train_time_s': t_elapsed_ffeb, 'model_path': ffeb_path, 'model_size_bytes': model_size_on_disk(ffeb_path)})
    results['models']['FFEB'] = convert_numpy(ffeb_metrics)
    print('\nFFEB metrics:', ffeb_metrics)

    # Profiling
    results['profiling']['gpu'] = get_gpu_memory()
    results['profiling']['system'] = get_system_usage()

    # inference timing on up to 100 test images
    X_sample = []
    y_sample = []
    try:
        gen_iter = iter(test_gen)
        collected = 0
        max_collect = min(100, test_gen.samples if hasattr(test_gen, 'samples') else 0)
        while collected < max_collect:
            imgs, labels = next(gen_iter)
            for i in range(len(imgs)):
                X_sample.append(imgs[i])
                y_sample.append(labels[i])
                collected += 1
                if collected >= max_collect:
                    break
    except Exception as e:
        print('Warning collecting samples for inference timing:', e)

    if len(X_sample) == 0:
        results['profiling']['inference_time_s_total'] = None
        results['profiling']['inference_time_s_per_image'] = None
        print('No test images found for inference timing.')
    else:
        X_sample = np.array(X_sample)
        _ = ffeb_model.predict(X_sample[:min(8,len(X_sample))])
        start_inf = time.perf_counter()
        _ = ffeb_model.predict(X_sample, verbose=0)
        inf_time = time.perf_counter() - start_inf
        results['profiling']['inference_time_s_total'] = float(inf_time)
        results['profiling']['inference_time_s_per_image'] = float(inf_time / len(X_sample))

    # Domain-shift evaluation
    ds_modes = ['brightness','blur','occlusion','noise','color_shift']
    ds_results = {}
    if len(X_sample) > 0:
        for mode in ds_modes:
            X_mod = np.array([apply_domain_shift(x, mode=mode) for x in X_sample])
            preds = ffeb_model.predict(X_mod, verbose=0).ravel()
            y_pred = (preds >= 0.5).astype(int)
            m = compute_metrics(np.array(y_sample[:len(y_pred)]), y_pred, preds)
            ds_results[mode] = convert_numpy(m)
    results['domain_shift'] = ds_results

    # Save results
    stamp = int(time.time())
    out_path = os.path.join(OUT_DIR, f'full_results_seed{seed}_{stamp}.json')
    with open(out_path, 'w') as f:
        json.dump(convert_numpy(results), f, indent=2)
    print('\nSaved results to', out_path)

    # Print compact summary
    print('\n=== MODEL SUMMARY ===')
    for k,v in results['models'].items():
        print('\nModel:', k)
        if isinstance(v, dict):
            for kk,vv in v.items():
                print(f'  {kk}: {vv}')
    print('\nProfiling:', results['profiling'])
    return results, {'individual_models': individuals, 'ffeb_model': ffeb_model}

# ----------------------------
# Quick helper to run and then test a custom image
# ----------------------------
def run_and_test_custom(custom_image_path=None):
    results, models_store = run_pipeline(seed=SEED, skip_training=SKIP_TRAINING, train_individuals=TRAIN_INDIVIDUALS)
    # models_store contains 'individual_models' list in order [vgg,resnet,inc] if trained
    mapping = {}
    names = ['vgg19','resnet50','inception_v3']
    for i,name in enumerate(names):
        try:
            mapping[name] = models_store['individual_models'][i]
        except Exception:
            mapping[name] = None
    mapping['ffeb'] = models_store['ffeb_model']
    if custom_image_path:
        print('\nCustom image prediction for:', custom_image_path)
        preds = predict_custom_image(custom_image_path, mapping)
        for k,v in preds.items():
            print(f'  {k}: {v}')
        return results, preds
    return results

# ----------------------------
# If executed directly, run pipeline
# ----------------------------
if __name__ == '__main__':
    # Example: set SKIP_TRAINING=True once models are saved to avoid retraining
    # To test a custom image after training, set custom_image_path to path in Drive or local
    custom_image_path = None  # e.g. '/content/drive/MyDrive/some_image.jpg'
    out_results = run_and_test_custom(custom_image_path=custom_image_path)
    print('\nDone. Output directory:', OUT_DIR)